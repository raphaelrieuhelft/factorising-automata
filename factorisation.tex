\documentclass[11pt, openany]{article}
\usepackage{pstricks,pstricks-add,pst-math,pst-xkey}
\usepackage[frenchb]{babel}
%\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext,amsthm}
%\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{pgf,tikz}
\usepackage{pgfplots}
\usepackage{floatpag}
\usepgfmodule{shapes}
\usetikzlibrary{arrows,patterns}
\usepackage[ps]{skak}
\usepackage{chessboard}
\newcounter{moncompteur}
\newtheorem{q}[moncompteur]{ \textbf{Question}}{}
\newtheorem{prop}[moncompteur]{ \textbf{Proposition}}{}
\newtheorem{df}[moncompteur]{ \textbf{Définition}}{}
\newtheorem*{df*}{ \textbf{Définition}}{}
\newtheorem{rem}[moncompteur]{ \textbf{Remarque}}{}
\newtheorem{theo}[moncompteur]{ \textbf{Théorème}}{}
\newtheorem{conj}[moncompteur]{ \textbf{Conjecture}}{}
\newtheorem{cor}[moncompteur]{ \textbf{Corollaire}}{}
\newtheorem{lm}[moncompteur]{ \textbf{Lemme}}{}
%\newtheorem{nota}[moncompteur]{ \textbf{Notation}}{}
%\newtheorem{conv}[moncompteur]{ \textbf{Convention}}{}
\newtheorem{exa}[moncompteur]{ \textbf{Exemple}}{}
\newtheorem{ex}[moncompteur]{ \textbf{Exercice}}{}
%\newtheorem{app}[moncompteur]{ \textbf{Application}}{}
%\newtheorem{prog}[moncompteur]{ \textbf{Algorithme}}{}
%\newtheorem{hyp}[moncompteur]{ \textbf{Hypothèse}}{}
\newenvironment{dem}{\noindent\textbf{Preuve}\\}{\flushright$\blacksquare$\\}
\newcommand{\cg }{[\kern-0.15em [}
\newcommand{\cd}{]\kern-0.15em]}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\card}{\mathrm{card}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\pgfplotsset{compat=1.8}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Ne}{\mathcal{N}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Ss}{\textsc{safestay}}
\newcommand{\Sg}{\textsc{safego}}
\newcommand{\M}{\textsc{move}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\V}{\mathcal V}
\setlength{\parindent}{0pt}
\newcommand{\myrightleftarrows}[1]{\mathrel{\substack{\xrightarrow{#1} \\[-.6ex] \xleftarrow{#1}}}}
\newcommand{\longrightleftarrows}{\myrightleftarrows{\rule{1cm}{0cm}}}

\newcommand\tikzmark[1]{%
  \tikz[overlay,remember picture,baseline] 
  \node[anchor=base](#1){};}

\newcommand\MyLine[3][]{%
  \begin{tikzpicture}[overlay,remember picture]
    \draw[#1] (#2.north west) -- (#3.south east);
  \end{tikzpicture}}





\begin{document}
\floatpagestyle{plain}
\renewcommand{\labelitemi}{$\bullet$}

\title{Multiplication binaire et factorisation}
\date{}
\author{}
\maketitle



\section*{Position du problème}


On cherche un système dynamique discret qui, étant donné un entier non-premier, trouve un de ses diviseurs non-triviaux. On se base sur la multiplication binaire. En effet, on observe qu'une matrice à coefficients dans $\{0,1\}$ peut s'interpréter comme une multiplication binaire si ses lignes non-nulles sont égales entre elles, comme illustré dans la figure $1$. Les colonnes non-nulles sont alors aussi égales entre elles, et les deux facteurs de la multiplication sont alors lisibles, en base $2$, sur les lignes non-nulles pour l'un, et de bas en haut sur les colonnes non-nulles pour l'autre. Si on pose leur multiplication, toujours en base $2$, on retrouve en effet la matrice de départ décalée d'un cran par ligne vers la gauche (cf. Figure $1$).




\begin{figure}[h]
\centering
\begin{minipage}[]{0.25\linewidth}

\begin{tabular}{cccc}
1&0&1&1\\
0&0&0&0\\
1&0&1&1\\
1&0&1&1\\
\end{tabular}

\end{minipage}
\quad
\begin{minipage}[]{0.4\linewidth}


\begin{tabular}{lllllllll|c}
&&&&&1&0&1&1&$\times$\\
\hline
&&&&&1&0&1&1&1\\
+&&&&0&0&0&0&.&0 \\
+&&&1&0&1&1&.&.&1\\
+&&1&0&1&1&.&.&.&1\\
\hline
&1&0&0&0&1&1&1&1&\\
\end{tabular}
\end{minipage}
\caption{La matrice de gauche s'interprète comme la multiplication de $11$ ($1011$ horizontalement) par $13$ ($1101$ verticalement, de haut en bas).}
\end{figure}

Par ailleurs, étant donné une matrice $M$ à coefficients dans $\{0,1\}$, on pose $\sigma$ l'entier obtenu en sommant ses lignes décalées (cf. Figure 2). Si $M$ représente une multiplication, $\sigma$ est par construction le produit. 

\begin{figure}[]
\centering
\begin{minipage}[]{0.3\linewidth}
$M=$
\begin{tabular}{cccc}
1&1&0&1\\
0&0&1&1\\
1&0&0&1\\
\end{tabular}
\end{minipage}
\quad
\begin{minipage}[]{0.6\linewidth}
$\sigma=$
\begin{tabular}{llllllll}
&&&&1&1&0&1\\
+&&&0&0&1&1&.\\
+&&1&0&0&1&.&.\\
\hline
&&1&1&0&1&1&1\\
\end{tabular}
$=55$
\end{minipage}
\caption{Exemple de calcul de $\sigma$.}

\end{figure}

L'idée est donc de partir d'une matrice $M$ à coefficients dans $\{0,1\}$ telle que $\sigma$ vaut le nombre qu'on cherche à factoriser, et de lui appliquer des transformations qui préservent $\sigma$ comme invariant jusqu'à atteindre une matrice qui représente une multiplication. Ainsi, notre système n'a pas à vérifier que le produit de cette multiplication est l'entier à factoriser, ce qui simplifie considérablement sa conception.

\section*{Un algorithme}

Soit $n$ le nombre de chiffres en base $2$ du nombre composé $N$ dont on cherche un diviseur non-trivial. On se donne une matrice $M$ de taille $ n-1 \times \lceil \frac{n}{2}\rceil$ telle que $\sigma=N$. Pour tous $a$ et $b$ différents de $1$ et $N$, et tels que $N = ab$, $a$ ou $b$ est de taille inférieure à $\lceil\frac{n}{2}\rceil$ et l'autre est de taille inférieure à $n-1$, on peut donc représenter leur multiplication à l'aide d'une matrice de la taille de $M$. On l'initialise en recopiant les $n-1$ bits de poids faible de $N$ sur la première ligne, et avec un $1$ en position $(2,1)$ et des zéros partout ailleurs (cf. Figure \ref{fig:init}). On vérifie aisément que $\sigma=N$. 


\begin{figure}[h]
\centering

$21 \Rightarrow \overline{10101}^2 \Rightarrow$
\begin{tabular}{cccc}
0&1&0&1\\
1&0&0&0\\
0&0&0&0\\
\end{tabular}
$\Rightarrow$
\begin{tabular}{ccccccc}
&&&0&1&0&1\\
+&&1&0&0&0&.\\
+&0&0&0&0&.&.\\
\hline
&0&1&0&1&0&1\\
\end{tabular}

\caption{Exemple de construction de la matrice $M$ de départ pour factoriser $N=21$.}
\label{fig:init}
\end{figure}

Formellement, on pose $\La=\cg 1, n-1 \cd \times \cg 1, \lceil\frac{n}{2}\rceil\cd$ l'ensemble des cases de la matrice, et $\E=\{0,1\}^\La$ l'ensemble des configurations. Pour tout $x \in \E$, $(i,j)\in\La$, on note $x_{i,j}$ le coefficient $(i,j)$ de la matrice $x$. 

On se donne également un ensemble $\Gamma \subset \E^{\E\times\La}$ de transformations locales qui conservent $\sigma$. Elles sont représentées sur la figure 3, et s'appliquent aux coefficients indiqués en gras sous certaines conditions.

%On se donne donc un ensemble $\Gamma$ de six transformations locales qui conservent $\sigma$. Elles sont représentées sur la figure $3$. Plus 


\begin{figure}
\centering
\[
\begin{tabular}{cc}
\textbf 1& \\
 &0\\
\end{tabular}
\mathrel{\mathop{\longrightleftarrows}^{\mathrm{R1}}_{\mathrm{R2}}}
\begin{tabular}{cc}
\textbf 0& \\
 &1\\
\end{tabular}
\]


\[
\begin{tabular}{cc}
0& \\
\textbf 1&0\\
\end{tabular}
\mathrel{\mathop{\longrightleftarrows}^{\mathrm{R3}}_{\mathrm{R4}}}
\begin{tabular}{cc}
1& \\
\textbf 0&1\\
\end{tabular}
\]

\[
\begin{tabular}{ccc}
\textbf 1&0&\\
&&0\\
\end{tabular}
\mathrel{\mathop{\longrightleftarrows}^{\mathrm{R5}}_{\mathrm{R6}}}
\begin{tabular}{ccc}
\textbf 0&1&\\
&&1\\
\end{tabular}
\]

\caption{Transformations locales préservant $\sigma$.}
\end{figure}


%À chaque pas de temps, on choisit au hasard une case et une transformation de $\Gamma$, et on décide si on applique ou non la transformation.


Une formulation équivalente de l'égalité des lignes non-nulles est qu'un coefficient situé sur la même ligne qu'un 1 et sur la même colonne qu'un 1 vaut nécessairement 1 dans une matrice qui représente une multiplication. On va donc appliquer aléatoirement des règles conservant $\sigma$, en privilégiant les mouvements qui retirent les zéros de telles cases. Dans la suite, on dira qu'une case est \emph{sûre} s'il n'y a pas de $1$ sur la même ligne ou s'il n'y en a pas sur la même colonne, et \emph{dangereuse} s'il y a à la fois un $1$ sur la même ligne et un sur la même colonne.

Pour diriger les zéros vers les cases sûres, on voudrait n'effectuer que des mouvements qui sortent un zéro d'une case dangereuse, mais cette restriction est trop forte : il existe alors des situations bloquées, la figure $4$ en donne un exemple. Il est donc nécessaire d'autoriser au moins certains mouvements qui ne sont pas directement utiles en ce sens. Par ailleurs, on pourrait vouloir restreindre les mouvements des zéros vers des cases dangereuses, de la même façon que pour le problème des $n$ reines. Cependant, il se forme alors des blocs stables de $1$ : les zéros qui viennent remplacer un $1$ du bord du bloc en sont rapidement éjectés (les seules cases sûres proches sont à l'extérieur du bloc). 

On prend donc le parti de ne pas restreindre les mouvements des zéros vers des cases dangereuses, et on se donne une probabilité d'agitation $p$ : un mouvement valide mais qui ne sort pas un zéro d'une case dangereuse est effectué avec une probabilité $p$, alors qu'un mouvement valide qui sort un zéro d'une case dangereuse est toujours effectué. 



\begin{figure}
\centering
\begin{tabular}{cccccccc}
0&1&0&0&1&1&0&\fbox 0\\ 
0&\fbox 0&0&0&1&1&0&1\\
0&\fbox 0&0&0&1&1&0&1\\
0&0&0&0&0&0&0&0\\
0&\fbox 0&0&0&1&1&0&1\\
\end{tabular}


\caption{Exemple de situation bloquée si on interdit les mouvements ne déplaçant pas les zéros menacés (encadrés) : aucune des règles de la Figure $3$ ne s'applique à eux.}

\end{figure}

À chaque pas de temps, on choisit donc une case de $\La$ et une règle de $\Gamma$ au hasard, et on applique éventuellement la règle à la case. À titre d'exemple, si on choisit, dans une configuration $x$, la case $(i,j)$ et la règle \textbf{R1}, la nouvelle configuration est \[
\textbf{R1}(x,(i,j))=
\begin{cases}
  \hfill x' \hfill & \text{ si}  \begin{cases}\text{ $x_{i,j}=1$ et $x_{i+1,j+1}=0$ }\\
    \text{ et}  \left\{
        \begin{tabular}{@{}c@{}l}
          &\text{ $(i+1,j+1)$ est une case dangereuse}\\
          \text {ou}&\\
          &\text{ $\mathcal{B}(p)=1$}\\
        \end{tabular}
      \right.
  \end{cases}
  \\
  \hfill x \hfill & \text{ sinon}
\end{cases}
\]

avec $x'$ la configuration obtenue à partir de $x$ en échangeant les valeurs de $x_{i,j}$ et $x_{i+1,j+1}$, et $\mathcal{B}(p)$ une variable aléatoire valant $1$ avec une probabilité $p$ et $0$ avec une probabilité $1-p$.

On applique une règle si au moins un des zéros concernés est sur une case dangereuse : ainsi \[
\textbf{R3}(x,(i,j))=
\begin{cases}
  \hfill x'' \hfill & \text {si} \begin{cases}\text{ $x_{i,j}=1$ et $x_{i-1,j}=x_{i,j+1}=0$} \\
    \text{ et} \left\{
        \begin{tabular}{@{}c@{}l}
          &\text{ $(i-1,j)$ est une case dangereuse}\\
          \text {ou}& \text{ $(i,j+1)$ l'est}\\
          \text {ou}& \text{ $\mathcal{B}(p)=1$}\\
        \end{tabular}
      \right.
    \end{cases}
  \\
  \hfill x \hfill & \text {sinon}
\end{cases}
\]

avec $x''$ la configuration obtenue à partir de $x$ en changeant les valeurs de $x_{i,j}$, $x_{i-1,j}$ et $x_{i,j+1}$.

\medskip

Du fait de l'existence de mouvements spontanés, il n'y a pas de point fixe à proprement parler : on est contraint de vérifier après chaque mouvement si la matrice représente une multiplication valide, et d'arrêter les mises à jour le cas échéant. 

\subsection*{Atteignabilité de la solution}

On cherche ici à montrer que la solution est accessible à partir de la solution de départ en appliquant uniquement les règles de $\Gamma$. On va en fait montrer un résultat un peu plus général : en appliquant les règles de $\Gamma$, il est possible de passer d'une matrice quelconque à n'importe quelle autre matrice pour laquelle $\sigma$ a la même valeur. 

\begin{df*}
Pour toutes matrices $A$ et $B$, on a $A\sim B$ si et seulement si on peut passer de $M$ à $N$ en appliquant des transformations de $\Gamma$.
\end{df*}

Remarquons que $\sim$ est une relation d'équivalence (la symétrie vient du fait que toutes les règles de $\Gamma$ ont un inverse, donc tous les mouvements sont réversibles).

Pour toute matrice $M$, on numérote ses diagonales descendantes en commençant à $0$ par celle réduite au coin supérieur droit (cf Fig. \ref{fig:diags}), et on note $b_i$ le nombre de coefficients égaux à $1$ sur la diagonale $i$, de sorte que $\sigma = \sum\limits_ib_i2^i$. On constate que deux matrices ayant les mêmes coefficients $b_i$ pour tout $i$ sont équivalentes pour $\sim$ (en fait, on peut passer de l'une à l'autre en utilisant uniquement les règles \textbf{R1} et \textbf{R2}).


\begin{figure}
\begin{tikzpicture}
\[
\begin{matrix}
\tikzmark{a} & \tikzmark{b} & \tikzmark{c} & \\
\phantom{0}\tikzmark{a21}& & \phantom{0}\tikzmark{a23}& b_0 \\
\phantom{0}\tikzmark{a31}& \phantom{0}\tikzmark{a32} &  \phantom{0}\tikzmark{a33} & b_1 \\
&b_4&b_3&b_2\\
\end{matrix}
\]
\end{tikzpicture}
\caption{Signification des coefficients $b_i$.}
\label{fig:diags}
\end{figure}

On va montrer que pour tout $N$, toutes les matrices de même taille telles que $\sigma = N$ et dont le nombre de coefficients égaux à $1$ est minimal ont les mêmes coefficients $b_i$, puis que toute matrice telle que $\sigma=N$ leur est équivalente pour $\sim$.  



\subsection*{Performances et observations}

Les performances varient grandement selon l'entier à factoriser : sa dé\-composition en facteurs premiers, et plus généralement la distribution de $1$ dans la matrice solution semblent être des facteurs déterminants. Ainsi, les nombres les plus faciles semblent être ceux qui ont le plus de diviseurs, ce qui s'explique par le fait que plus de solutions existent pour de tels nombres. Si on ne s'intéresse qu'aux nombres qui sont le produit de deux nombres premiers, les nombres pour lesquelles les matrices solutions du problème comportent peu de $1$ semblent les plus difficiles : l'algorithme trouve en $10^6$ pas de temps en moyenne un diviseur de $493$ ($ = 17\times 29$), mais pour factoriser $362$ ($ = 2\times 181$), il lui faut $10^9$ étapes ! Ceci est dû au fait que la densité de $1$ varie très peu au cours du temps (elle oscille autour d'une densité moyenne, et les solutions pour lesquelles la densité finale en est éloignée sont difficiles à trouver). 

\medskip

De plus, les conditions que l'on s'est donné pour appliquer les règles tendent à chasser les zéros menacés, mais rien ne pousse particulièrement les $1$ isolés à se déplacer. Il est donc relativement difficile de trouver les positions pour lesquelles les $1$ sont tous à un endroit précis de la matrice : ainsi, pour les nombres de la forme $2^kp$ avec $p$ premier, la matrice aura tous ses $1$ sur la même ligne (la $k$-ième) ou bien sur la même diagonale. Ainsi, dans les cas ``faciles'' tels que $493$, une solution est atteinte environ $40$ fois plus vite avec $p=0.05$ qu'avec $p=1$ (cas où le système applique des règles valides sans tenir compte du caractère dangereux ou sûr des cases), mais dans les cas difficiles tels que $362$, les performances sont plutôt meilleures avec la marche aléatoire $p=1$. Plus généralement, l'évolution du système n'est dirigée que faiblement, notamment parce qu'on a décidé de ne pas restreindre les déplacements de zéros vers des cases dangereuses et parce que le système n'a pas de mémoire. 

\medskip
 
Le fait que les $1$ isolés ne soient pas encouragés à se déplacer est d'autant plus gênant qu'on a choisi une taille de matrice nécessairement trop grande pour les facteurs que l'on va trouver. En effet, on a pris une matrice de taille $n-1 \times \lceil \frac{n}{2} \rceil$ pour un nombre de taille $n$ afin d'assurer de pouvoir trouver n'importe quel diviseur non trivial, mais la somme des longueurs des deux facteurs que l'on va trouver est au plus $n+1$. Par conséquent, tous les $1$ d'une matrice solution sont contenus dans un rectangle dont l'un des sommets est le coin supérieur droit et dont le périmètre est limité : s'il y a un $1$ dans le coin inférieur droit, il ne peut y en avoir aucun dans toute la moitié gauche de la matrice ! Si, dans une exécution de l'algorithme, aucun $1$ n'apparaît typiquement dans le quart inférieur gauche pour $p$ assez petit, il y en a fréquemment à la fois dans le coin supérieur gauche et inférieur droit, ce qui fait perdre beaucoup de temps. 

\subsection*{Une amélioration}

On peut donc envisager l'amélioration suivante : plutôt que d'utiliser une grande matrice, on suppose connue la taille des diviseurs. Si $n$ est la taille du nombre à factoriser, il y a au plus $2n-1$ tailles de matrices possibles à essayer car la somme des tailles des diviseurs vaut nécessairement $n$ ou $n+1$ et on ignore les matrices dont une dimension vaut $1$ car on cherche des diviseurs non-triviaux. On lance donc en parallèle l'algorithme sur les $2n-1$ matrices différentes, et on arrête l'exécution dès qu'une solution est trouvée pour l'une des tailles. Quitte à supposer impair le nombre à factoriser, on impose de plus qu'il y ait un $1$ à chacun des quatre coins (c'est une condition nécessaire pour qu'une matrice dont les dimensions sont exactement les tailles de deux facteurs soit solution : chaque facteur commence par un $1$ car les zéros à gauche ne sont pas significatifs, et finissent par un $1$ car les diviseurs d'un nombre impair sont impairs). 

\medskip
Il apparaît que l'exécution de l'algorithme est significativement plus rapide qu'avec la grande matrice. Les nombres dont l'un des facteurs est très petit deviennent très faciles à traiter, car la matrice de la bonne taille est très petite, et le calcul y aboutit donc rapidement. Les nombres les plus difficiles à traiter deviennent, comme pour les algorithmes classiques de factorisation, les produits de deux nombres premiers de même taille : en effet, la seule matrice pour laquelle l'algorithme peut aboutir est alors de taille maximale. Même dans ce cas, l'exécution est plus rapide car on ne perd plus de temps à cause des $1$ situés dans une zone inutile de la matrice. On gagne ainsi un facteur $100$ sur le temps de factorisation de $329$ ($7\times 47$), largement de quoi justifier de lancer simultanément une quinzaine d'exécutions. % TODO : stats plus détaillées ici ?

\end{document}
